{"cells":[{"cell_type":"markdown","source":"# COMP 551 A3\nEsther Chen, Grace Hu, Kristen Peng","metadata":{"tags":[],"cell_id":"00000-63cb4f9a-7a9e-4f8c-beae-71196087f4ea","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00001-6d09b931-bea3-4a76-879b-71af92456cf4","execution_millis":1138,"output_cleared":false,"source_hash":"c5ac8128","tags":[],"execution_start":1607991530625,"deepnote_cell_type":"code"},"source":"import os\nimport copy\nimport h5py\nimport pickle\nfrom glob import glob\nfrom random import shuffle\n\nimport cv2\nfrom PIL import Image\nfrom IPython.display import SVG\n\nimport numpy as np\nimport pandas as pd\n\nimport sklearn.utils\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00002-ab5aa61e-9e36-47bc-aec8-e638a845c1f0","execution_millis":3064,"output_cleared":false,"source_hash":"cbd85a4","tags":[],"execution_start":1607991536264,"deepnote_cell_type":"code"},"source":"import tensorflow as tf\nprint(\"Tensorflow version: \" + tf.__version__)\nimport keras\nimport keras.backend as K\n\nfrom keras.models import Model\nfrom keras.layers import Input, BatchNormalization, Convolution2D, Dense, Dropout, MaxPooling2D, Flatten\nfrom keras.layers import AveragePooling2D, GlobalAveragePooling2D, concatenate, UpSampling2D, Conv2DTranspose\n\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.utils.vis_utils import model_to_dot","execution_count":7,"outputs":[{"name":"stdout","text":"Tensorflow version: 2.2.0\nUsing TensorFlow backend.\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"cell_id":"00007-b8ec6b8f-52dc-4cad-971c-69dd3b7b7f50","execution_millis":3,"output_cleared":false,"source_hash":"da58e4e","tags":[],"execution_start":1607990480696,"deepnote_cell_type":"code"},"source":"def get_model(input_shape=(28, 28*5, 3), p=0.5, n_class=11):\n\n    inputs = Input(((input_shape[0], input_shape[1], input_shape[2])))\n    \n    x = BatchNormalization()(inputs)\n    x = Convolution2D(48, 5, activation='relu', padding='same', strides=(1, 1))(x)\n    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n    x = Dropout(p/4)(x)\n    \n    x = BatchNormalization()(x)\n    x = Convolution2D(64, 5, activation='relu', padding='same', strides=(1, 1))(x)\n    x = MaxPooling2D(pool_size=(2, 2), strides=(1, 1))(x)\n    x = Dropout(p/4)(x)\n\n    x = BatchNormalization()(x)\n    x = Convolution2D(128, 5, activation='relu', padding='same', strides=(1, 1))(x)\n    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n    x = Dropout(p/2)(x)\n\n    x = BatchNormalization()(x)\n    x = Convolution2D(160, 5, activation='relu', padding='same', strides=(1, 1))(x)\n    x = MaxPooling2D(pool_size=(2, 2), strides=(1, 1))(x)\n    x = Dropout(p/2)(x)\n\n    x = BatchNormalization()(x)\n    x = Convolution2D(192, 5, activation='relu', padding='same', strides=(1, 1))(x)\n    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n    x = Dropout(p)(x)\n\n    x = BatchNormalization()(x)\n    x = Convolution2D(192, 5, activation='relu', padding='same', strides=(1, 1))(x)\n    x = MaxPooling2D(pool_size=(2, 2), strides=(1, 1))(x)\n    x = Dropout(p)(x)\n    \n    # I had to remove this part because the input size we have is too small for a network this deep.\n    # Another alternative would have been change the maxpool strides.\n    \n    #x = BatchNormalization()(x)\n    #x = Convolution2D(192, 5, activation='relu', padding='same', strides=(1, 1))(x)\n    #x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n    #x = Dropout(p)(x)\n    #x = BatchNormalization()(x)\n    #x = Convolution2D(192, 5, activation='relu', padding='same', strides=(1, 1))(x)\n    #x = MaxPooling2D(pool_size=(2, 2), strides=(1, 1))(x)\n    #x = Dropout(p)(x)\n    \n    x = Flatten()(x)\n    x = Dense(1024, activation='relu')(x) # I also reduced the number of activations\n    x = Dense(1024, activation='relu')(x)\n    \n    c1 = Dense(n_class, activation='softmax')(x)\n    c2 = Dense(n_class, activation='softmax')(x)\n    c3 = Dense(n_class, activation='softmax')(x)\n    c4 = Dense(n_class, activation='softmax')(x)\n    c5 = Dense(n_class, activation='softmax')(x)\n    \n    output = [c1, c2, c3, c4, c5]\n    \n    model = Model(inputs=inputs, outputs=output)\n\n    return model\n","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00008-9f11d07a-4d52-44c0-8f92-d4568f80f1c7","execution_millis":1,"output_cleared":false,"source_hash":"7e4ad331","tags":[],"execution_start":1607990484733,"deepnote_cell_type":"code"},"source":"def convert_output(model_output):\n    model_output = np.array(model_output).swapaxes(0, 1)\n    labels = []\n    for output in model_output:\n        label = convert_label(output)\n        labels.append(label)\n\n    return labels","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00009-9b2631d7-aef3-44a5-82a9-3a2730a2300e","execution_millis":319,"output_cleared":false,"source_hash":"ffa0172d","tags":[],"execution_start":1607991549213,"deepnote_cell_type":"code"},"source":"h5f = h5py.File('MNIST_synthetic.h5','r')\n\nprint(h5f.keys())\n# Extract the datasets\nX_train = h5f['train_dataset'][:]       # remove\ny_train = h5f['train_labels'][:]\nX_test=h5f['train_dataset'][:] \ny_test=h5f['train_labels'][:]\n#X_val = h5f['valid_dataset'][:]\n#y_val = h5f['valid_labels'][:]\nX_kaggle = h5f['test_dataset'][:]\n#y_test = h5f['test_labels'][:]\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\nX_train = h5f['train_dataset'][:]       # remove\ny_train = h5f['train_labels'][:]\n# Close the file\nh5f.close()\n\nprint('Training set', X_train.shape, y_train.shape)\nprint('Validation set', X_val.shape, y_val.shape)\nprint('Test set', X_test.shape)","execution_count":8,"outputs":[{"name":"stdout","text":"<KeysViewHDF5 ['test_dataset', 'train_dataset', 'train_labels']>\nTraining set (56000, 64, 64, 1) (56000, 5)\nValidation set (11200, 64, 64, 1) (11200, 5)\nTest set (56000, 64, 64, 1)\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"cell_id":"00010-916e36fb-7cdc-4221-9451-aeb9c23904cb","execution_millis":388,"output_cleared":false,"source_hash":"a27df78d","tags":[],"execution_start":1607990495094,"deepnote_cell_type":"code"},"source":"def plot_images(images, nrows, ncols, cls_true, cls_pred=None):\n    \n    # Initialize figure\n    fig, axes = plt.subplots(nrows, ncols, figsize=(16, 2*nrows))\n    \n    # Randomly select nrows * ncols images\n    rs = np.random.choice(images.shape[0], nrows*ncols)\n    \n    # For every axes object in the grid\n    for i, ax in zip(rs, axes.flat): \n        \n        # Pretty string with actual number\n        true_number = ''.join(str(x) for x in cls_true[i] if x != 10)\n        \n        if cls_pred is None:\n            title = \"True: {0}\".format(true_number)\n        else:\n            # Pretty string with predicted number\n            pred_number = ''.join(str(x) for x in cls_pred[i] if x != 10)\n            title = \"True: {0}, Pred: {1}\".format(true_number, pred_number) \n            \n        ax.imshow(images[i,:,:,0], cmap='binary')\n        ax.set_title(title)   \n        ax.set_xticks([]); ax.set_yticks([])\n        \n        \n# Plot some images from the training set\nplot_images(X_kaggle[:10], 1, 10, y_train[:10])\n        \n","execution_count":null,"outputs":[{"data":{"text/plain":"<Figure size 1152x144 with 10 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAA44AAABtCAYAAADuzz8lAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAp3ElEQVR4nO3deXQV9f3/8ecn997sC2QPJhD2TVbBtCKbqEXRnxUF7Ff9auuvttrlp7+eb631++Oo7fl2Oe2pX6vf1lpbW0VbXCpLRVAUi7IUKEtYZEtCEpKYnZD9LvP7497chpBMgoWbC3k9zrnnJJn5zJ15zyefmffMZz5jLMtCREREREREpCcR/b0CIiIiIiIiEt6UOIqIiIiIiIgtJY4iIiIiIiJiS4mjiIiIiIiI2FLiKCIiIiIiIraUOIqIiIiIiIgtJY4iIiIiIiJi67wkjsaYxk4fnzGmpdPvd56P7ziHdfk3Y8wJY0yTMeYtY0xyN/OMNsa0GmNe7vL3bxljCo0xDcaYncaYq7spG2mMOWSMKb2Q29EX4RT3Tuv0O2OMZYwZ1elvLxtjygNxPWKM+d89lF0eKHtt6NY4+N1hE0u7emiMGWSM+YMxpjLwebxL2auMMX83xpw2xuzrWod7Wfa6LnFoN8bkX4DtC4tYG2OyjDGrjTFlgXqX22X6gS7r6jHGrOk0/WZjzP7AtC3GmAmdpl1ujFlvjKk2xpz1slxjTK4x5m1jTJ0xpsIY84wxxnkBtjEsYh1Yl0u6XnezveEU+0v2uKg4h064xNoYMy/w/Z3X555O03s85zDGTAjEty7wea9L2/24McbdZdkjQrVtn0UY7ZdFxpiPjDH1xn9c+60xJiFU3x8KijVgWdZ5/QBFwLU9THOe7+/rsvyJwGlgDhAPvAL8qZv5NgCbgZc7/S0PaAKuAAzwAFAFOLqUfQz4G1B6IbflYop7p++5GvgQsIBRXfZLVODncUAFcEWXsiOBfKCsp+0YCLHsrR4CvwdeA2KBXOA48OXAtGSgBlgCOIC7gDpgcF+W3c26bAKWX8KxzgAeBD4fqLO5NvMaoBD498Dvo4GGQJ13Ao8CxzrWGRgL3Afc4m9mz1re28CLQDSQGaj7376EYz2g6nWYxX7AHBcV5wET63l2McDmnAMYFGhjTKA9+Tawr1PZxzvvm4vt08/75d+AhYF2fDCwDvh1f8dEsT6/nwvaVTVwVajUGPOIMaYC+L0x5l5jzEdd5gveoTLGRBljfmaMKTbGfGqM+bUxJqaPX3knsMayrL9ZltUI/D9gcecs3BhzB1APbOxSNhc4YFnWLsu/V/4IpALpncoOx3/S8qO+RyH0+iHuGP/dkl8C3+o6zbKsA5ZltXX8GviM7DLbs8AjQHufNzQE+iGWudjXw5uBn1qW1WxZVhHwAvCVwLSrgArLsl6zLMtrWdbL+E8+Fvdx2Z23JxeYHZgnJEIda8uyPrUs63+AHX2YfQ7+WL0R+P0LwGbLsj6yLMsD/AS4DJgbWPZhy7JeAA70sLzhwErLsloty6oA3sF/shMSqtehq9fdrIOOiyGgOIdOf5xz2LE757Asq96yrKJAnA3gBUZ1v6SLWz8cU1+xLOudQDteBzwPzDrvGxaGBlKsQ/GMYyb+K8bDgPv7MP+PgTHAVPz/zJcByzsmGv9t2bO6cARMBPZ2/GJZ1nH8iciYQNlE4Eng/3ZTdh3gMMbkGWMc+E9a9uC/UtXhl8D3gZY+bEd/C2XcAR4G/mZZ1r7uJhpj/scY0wx8ApTjv+PSMW0J0GZZ1tvdlQ0DoYxlX+qh6fLz5T1M6zq9L8vu8O/4E6OiHtbzQgl1ve2re4A3LMtq6vS3rvuh676w8xRwhzEm1hhzGXAD/uQxlFSv+4+Oi6GhOIdOqNvu9MDJdqEx5hfGmLjOE+3OOTqWD7Tij+t/dVn2zcaYWuN/XOGBPmxLOOvPY+ocer54eikaGLG+ALdPiwjcusXfnaAdiO40/V7goy5lLPxBM/i7a4zsNO3zQGEfv3sj8PUufzsJzAv8/N/AI4GfH+fMriIGf6PsBjxANTCz0/RbgXWdtqvfu4qEUdxz8HfTS+q83G7mc+Dv2vefgCvwtwTgKIFugtjc+h8gseytHr4MvBmI2yj8XfraAtNS8F/N/hLgwp/s+IDn+rLsLutxDLj3Uo51pzJObLqq4u8K0kCgHQn8bVzgu+cBkfjvLviAR7uUHUX3XVXHA7sC+8HC323VXKqxHmj1OsxiP2COi4rzgIl1JjAB/82P4fi77z7XzXxnnXN0mR6H/3GFRZ3+NgEYEih7Ff6k80v9He+LYb90WeZ1+B8pGNPfMVGsz+/nvA/G0I0qy7Ja+zhvGv6TtF3GBC8wd/RD74tGILHL3xKB08aYqcC1wLQeyt4HfBn/VcNjwPXAWmPMNOAU8FPgxj6uRzgIZdyfAp60LOuU3UyWZXmBj4wxd+F/huNp/AfQl6z+vwNgJ5Sx7LEeWpZVhv95jF/iT7ZrgFfxn1BjWVaNMeYW4Gf4u/6uB94DSvu4bP/K+q9wZQKv93Gdz6dQxrqvFgO1+J/fBcCyrE+MfzCGZ4As/InPQf4Z6x4ZYyLw3138Df4Tk3jgd/i7u373PK+7HdXr/qPjYmgozqETslhb/u79HXdjC40x3wXWAl/rMl935xydpzcZY34NVBljxluWVWlZ1sFOs2wxxvw3cDv+NuliFPJjqjHmc/if8b3dsqwj51L2IjcgYh2KxNHq8nsT/mABYIzJ7DStGn83jImWZZ38DN91AJjSadkjgCjgCP6GORcoDuykePxdQyZYljUd/63itZ0C/44xphz/id2xQNnNgbKRQFKgH/PnwjTpCWXcFwBXG2N+2ulvW40x/8eyrFe6md/JP59xXABkG2MeDPyeBqw0xvzEsqyffIZ1uRBCGcup9FwPX7csqxb/szQd3/1fwN+DK2pZHwIzA9OcQAHw874su9M63AO8afmf0wm1UMa6r+4B/mgFLu11sCzrdQJxM8YMwt/G9OV5yWRgKPCM5X8Op80Y83vgh4Q2cVS97j86LoaG4hw6/dl2W9g/etX5nKOrCPzreRlQ2cOyu3aVv5iEdL8ELnasBr5iWVbXZ3kvdQMj1uf7FiZn37ot7TJ9DNCGv6GMBn5Np66N+LtzrATSA79fBnyhj989EX+Xstn4uyC8TGBUM/w7L7PT52f4TyrSAtPvwd/Aj8DfSFwHNOPvkubsUnYx/tE/M+lh5L5Qf/o57uld4mMBnwNiAtPuIHCgxD+oSBPwvwJlU7qULcE/emL8AI1lj/UwMH1kIGYO/M/FVeNveDrKT8PfnS8R/53gj/u67MA8MfivcF9zqcc6MH80/rbCwj8SanSX6dn4u46N7KbsFYH9kBZYh1c6TTOBZU8ILDuawCh/gekFwPfwty2DgL90Ln+pxXqg1eswi/2AOS4qzgMm1vPxP0dm8D8q8wHw+8C03s45rsPfnjjwtydPB+IZHZh+C/5RKg1wJf7uxvf0V5wvsv1yOfApsKy/46BYX8DtDnUgA39/DP+JQQn+UcI6BzIa/4PKBfgb4UN0GqYef3eQ2Tbf/29AcaChWAUk9zDf45z9jMGTgbKnA997dw9lu92ucK7AFzruXb6n83LT8Hfxqw8sNx/4al+2YyDGsrd6CCzFf5Brxj94whe6lH8V/wnyKeDPBBqkvtZx/N0DT3CBn7cLh1h3qqtnfLpMfxT/YCrdlf0oEMda4DkgrtO03G6WXdRp+lT8r4WoC2zbSiDjUo31QKvX4RT7wPQBcVxUnAdGrPEPMHQSf3tRgj/5SwhMsz3nwH9h+pPA8quAvwKTO01/FX93+cbAfBf0NUmX2H75Pf7nzxs7fQ70d0wU6/P7MYEVEBEREREREelWKF7HISIiIiIiIhcxJY4iIiIiIiJiS4mjiIiIiIiI2FLiKCIiIiIiIraUOIqIiIiIiIgt57nMnJqaauXm5l6gVbm0FBUVUV1d/ZleGqs4n5tdu3ZVW5aV9lnKKtbnRrEOHcU6dBTr0NBxMXRUp0NHsQ4dxTo07Nrqc0occ3Nz2blz5/lZq0vcjBkzPnNZxfncGGNOfNayivW5UaxDR7EOHcU6NHRcDB3V6dBRrENHsQ4Nu7ZaXVVFRERERETElhJHERERERERsaXEUURERERERGwpcRQRERERERFbShxFRERERETElhJHERERERERsaXEUURERERERGwpcRQRERERERFbShxFRERERETElhJHERERERERsaXEUURERERERGwpcRQRERERERFbShxFRERERETElhJHERERERERsaXEUURERERERGwpcRQRERERERFbShxFRERERETElhJHERERERERsaXEUURERERERGwpcRQRERERERFbShxFRERERETElhJHERERERERsaXEUURERERERGwpcRQRERERERFbShxFRERERETElhJHERERERERsaXEUURERERERGwpcRQRERERGUB8Pl9/r8KAcSnFWomjiIiIiMgA4fF4OHnyJO3t7f29Kpe8Sy3WYZ04+nw+GhsbKSoqoq2trb9XZ0BqbGyksLBQ8e+BZVm0tLTwySef0NDQgGVZfS7r8/koKCigtbX1Aq7hpc/j8dDU1ER9fT1er/ec9sG5siyLhoYGSktLKSgooKamBrfbfcG+r7+oXocvHRdDQ3Huf263m/r6eoqLiykvL/9Md22OHDnC7373O/bs2YPX670Aa3nxaWlpYc+ePcyZM4fDhw+rfl9Al2Ksnf29AnYqKyv58MMPee655xg6dCjLly8nJycHl8vV7fyWZVFeXs7JkydJS0sjJSWFyMhInE4nDocjxGt/8WttbWXTpk388pe/ZPz48dx0001MmTKFwYMH43SGddUJiba2Nvbt28err77Knj17yMjI4Otf/zpTp04lKSnJtqzP56O4uJhHHnmEJ598ktGjR/caU4/HQ0SE/1pPa2srHo8HYwxOp5OYmJhuv+PUqVN89NFHfPzxx8EToOzsbG677TZmz54dVv8X9fX1FBYW0t7eTl5eXp/LVVZWsm/fPvbv3095eTkPPfQQmZmZPbYTHdra2jh69Cjvv/8+lmVx+eWXM27cOLKysoJx7szn81FbW8ujjz7K/v378Xq9DB8+nKlTp/L5z3+eq6+++qx92JGAvfzyy2zcuJHY2FgSExPJyclh3rx5zJgxo8/bGSqq1+FNx8XQUJzP5vF4AHA4HFiWRWVlJXv37mXs2LEMGzYMY4xt+ba2NkpKSli1ahUOh4P77ruP+Pj4Hsvt3LmTbdu2cfToUYqLi/ne977HzJkziYqK6nVdLcuiqamJn/zkJ5w8eZKYmBhGjhxJQkJCj2W8Xm/Y7yu3283GjRvZv38/qampTJkyhWnTpgWnt7a2EhERgdPp7PY45na7OXjwIE899RTDhg0jKyvL9lhpWRaNjY3U19cH939nERERREVFkZmZecbf29vbqampobi4GJ/Ph8vlIikpiba2Npqbm8nKyiInJ+dfiMSFp1h3L6zP/uPi4hgxYgR5eXn85S9/4Z133mHZsmWkpKT0WObUqVPs2rWL/fv3Ex8fz+DBg8nJyWHy5MmMHTu215NJ+aeysjLy8/PZsWMHJSUlHDlyhJkzZ3LTTTcxYcIE4uLi+nsV+9XBgwfZtm0bLS0tLF68mIKCAtatW0dycjKTJk2yLdve3s7GjRspLy8Pnih3x+v1curUKUpKSti1axdJSUmcPn2a4uJiTp06hcPhICcnh6997WtERkYGy7ndbsrLy3n99ddZt24dRUVFwcYoPT2d6OhoPve5z4XVQdLhcFBZWUltbe05JY6nT5+mrKyM1tZWtmzZwpe//GXS09Nty5SXl7NlyxZWr17N/v37iYiIYMiQIcyaNYuFCxcyefLkbsu53W727t1LaWkp8+bNA2DHjh0UFxczefJkkpOTz5jfGIPD4SA7O5vs7GwcDgeNjY2cOHGCo0ePhmXiqHod3nRcDA3F+UxtbW2sXr2a+Ph48vLyiIuLo6CggBdffJG7776boUOH2iaOHo+H/Px83n77bfLz84mIiCAnJ4c5c+aQmprabdmsrCxmzJhBVlYWK1eu5M9//jNjxowhLS2t1yTV7Xbz0UcfsXPnTr74xS8ycuRIIiIisCyr27Jer5empiYSExNxu92cPn2ayMhIIiMjaWxspKKigsTERDIyMvp1PzocDjIyMlixYgXjx49nypQpwWl1dXWsXr2aadOmMXr06LMuvFmWFbxYeuzYMb761a+SkJDQbdLTwePxsH//ftatW8fJkyfPWA+Hw4ExhujoaG666SbmzZsXXFZFRQUbNmxg06ZNwXnBfwfOGMM111zDfffd1+t+7E+KdffCOnGMiYkhNzeXBQsW8Ic//IHNmzezaNGiHhtuYwyxsbEkJSXR1NRETU0N+/btIzIykqqqKqKiohg9enSIt+Li9emnn1JWVobT6WTixIns3buXgoIC2traaGlp4YorrhiwyWN9fT07duygoKCAGTNmsHjxYtatW8fKlSupra21Levz+aivr2fDhg1MmDDBtjGpqqoiPz+frVu38o9//IPRo0fjcDj49NNPOXjwIPX19cyaNeusroQtLS0UFBSwatUq8vPzcblcNDY2BrsPVlZWnp9AnEcul4vW1lbq6ur6XKalpYXi4mKOHTvG8OHDqaiowO1223atrKqqYvv27axYsYKNGzeSkpJCdnY2paWlbNq0CZfLxfjx4886OTDG4HK5yMzMpKysjFGjRlFcXExhYaHt90VGRjJ9+nQSExNpa2tj165dlJaWhuVde9Xr8KfjYmgozv/k8Xj49NNPeeWVV5g/fz5TpkwhKiqKyspKtmzZwvXXX99rd/ba2lr27t3LP/7xD8aPH8/+/fvZvXs3U6dOJTU1tdsy2dnZDBkyJHgX5je/+Q0VFRUkJSXZ3nXsuNu4atUqIiIiyM3NpbKykvr6ejIzM5k4cWKPF5e8Xi/bt2+nsbExeCepvr6eoqIiJkyYQFJSUr9fAGhpaQkmuR2xc7vdfPjhh+zdu5eRI0d2W662tpatW7eybds2xo8fzw033IDT6Qz2+uipvbYsK/hISIeIiAgaGxs5efIkZWVlZGRkMGfOnOAyGhsbKSgooLS0lLFjx+Lz+di1axdlZWXExcWRmpqKx+Pp91j2RrE+W/iduQT4fD6qqqooKSmhrq4On89HdXV1t7dvO+u4HTx37lxKS0tZs2YNf/7zn2lpaSE5Ofmibbj7Q319PY2NjYwYMYIf/ehH/OY3v+Gll17ixRdfpK6ujuTkZC6//PL+Xs2Q8/l85Ofns2fPHmJiYrj55psZNGgQiYmJJCQk9PrP2draSnFxMXv27OHpp5/u8Wor+O/6bt++nc2bNzNo0CBSU1OZOHEiXq8Xl8vFJ598Ql5e3lkH0ba2Nurq6nC73UydOhWAAwcO0NbWRkJCAtOnT+9Td59QMsZgWVbwOcXero55vV6OHz/Otm3b2LNnD1dffTXNzc22Zdra2ti+fTtvvPEG7733HnFxcdx2223Mnz+fPXv28Le//Y21a9dy9913n3UyY4whPj6eW2+9lW3btrFv3z7Ky8uJiYnhuuuuY/DgwT1uV2ZmJpmZmVRVVXH48GE8Hg9Dhw7F5/PZXoEMJdXr8KfjYmgozmdqbW3l8OHDbNq0ibvvvpuYmBg8Hg+tra20t7f36YS0pKSE0tJSkpKS+NrXvsbjjz9OXFwcLperx3bC6XRiWRbx8fHceOONPPXUUxQXF5OdnW37f+7xeKiqqmLNmjVMnz492Cukra2NmTNn8t3vfpdBgwadUcbhcJCQkEBVVRXPPvssgwYNwul00tbWRnt7O4WFhbhcLmbOnHlOsTufOroyvvbaa6SnpzNhwgSysrLwer1UVlby29/+lkWLFpGTk9NtN/8DBw6wadMmqqur+c53vkNycjJFRUW0t7eTnJzM4MGDz+jhAf4Lunl5eUyaNIna2losy8Ln8+F2u9mxYwdr1qyhqKjorH0YGxvLqFGjiImJ4Z577gHgxz/+Mfn5+Vx22WWkpKSEdeKoWPcsbBPH06dP86c//YkXXniB8vJyamtr+fa3v23bP71DZGQkQ4YMYciQIeTm5rJv3z5aWlo4depUCNb80uD1ejl58iRVVVWkp6eTm5vL8uXLqa+v5+233+bvf/8777777oBMHOvq6njmmWdISUnh2muvJT09HcuyqK2tZc6cOWRlZfVafvv27WRkZDBjxgxiY2N7nHfatGlMnDiRhx9+mNjYWIwx1NbW8sQTT1BRUcE111zD0qVLzyqXlpbG9ddfT3x8PE1NTTz33HN4PB6ioqIYOnQoN998c9h1ETHGBNepL4ljZWUlTz/9NEePHiUvL4+UlBSam5tty+Xn5/OnP/2J9evXk5aWxkMPPcT9999PVFQUERER7N27l4MHD1JeXt7tVfDo6GiWLVvG888/z/r165k/fz533XUXX/ziF3tdX5/Px6pVq1i7di11dXWMGTOG4cOH96nbVSioXoc/HRdDQ3E+k9vtprS0lLi4OCZPnkxMTAxlZWWUlZXR3t5OSkpKn9q/jmUdPHiQyspKnnjiCYYMGWJbpqMXSkVFBc3NzZw+fbrXAcmampo4cuQIlZWVvPPOO8TFxTF16lSys7OpqKhg9+7dzJ8//6xybW1tvPjii3z44YcsXLiQhQsXMmPGDNxuN3fffTfz5s0jPj6+DxG7MDweD3v37uXtt9/mV7/6FVdeeSVOp5NTp06xbt06mpubWbBgQbcxbW9v57333qO6uppp06Zx+eWXs3v3br7xjW/Q1NTEsmXLWLJkSbePI3Qk1R31v76+nq1bt7JmzRo2b97M3Llz+frXv35GL5q0tDQmTpzIiy++iMPhIDc3l/LycmbNmsV//Md/kJycHNZttWLds7BNHJOSkvjKV77CwoULyc/P51e/+hWHDh2isbGR1NTUPl2ltyyLdevWceTIEfLy8rjqqqtCsOaXBq/XS21tLc3NzcGH3uPj43nssceoqalhz549HDp0qL9Xs1+sWLGCqqqqYD9zy7I4ffo0L730Ek888USP3W7gnwn5hg0buPPOO0lKSrKty8YYoqKigldX29vbefbZZ6mpqWHx4sUsXryYxMTEs8p1dNVZt24dzz//PM3NzcG7W1VVVXz88cdkZmbantyHmmVZuN1u2tvbe70T197ezq9//Wu2bNnCoEGDMMbwwgsv4HQ6iYyM7LZsTU0NP/vZz9iyZQvTp0/n0UcfDQ5o09bWxo4dOzh8+DDx8fFnXY3u0Nrayl//+lcKCwtJTEykvLw8eAexNz6fj7Fjx/Lwww/j8XjYt28f3/zmN4PJWn8/l6d6Hf50XAwNxflMsbGxzJw5k9GjR7N8+fJg2/fJJ5/gdDoZOXJkryem48ePJz8/nzfffJPvfOc7rFy5Mvjcd3csy2L79u2sX7+e999/n6NHj1JbW8uHH35Ibm4uiYmJ3d7pAf9Jf0eCmZuby+OPP84111xDVFQUhw4d4sCBA8ybN++MdW5paSE/P58XXniBH/7whyxcuJDMzEwqKyt5//33mTJlCqNGjerXHg1er5dDhw4FBxRyu900NDRw6NAhli9fzgMPPMD+/ftJSEg460Lfp59+ys6dO8nKyuL6669n586dPPDAAyxbtozTp09TUFDA+vXrbZ9j93q97N27l8cff5zt27eTmZnJvffey4MPPnhWex0XF8cVV1zByy+/zEMPPcRTTz3Ftddey9y5c22fEw4XinXPwjZxBEhMTCQ2NpaMjAwqKyt58sknufXWWxk0aNBZg1B0p7q6mpdffpmoqCimTp3K2LFjQ7DWlwaHw0F8fDxer5ejR4/S0NDA4MGDSU1NJSkpKdh9baBpbW3lgw8+YPHixeTl5REdHU1TUxOrV69mwoQJDB8+3Pa5z/b2durq6qirqzvn0R+9Xi9vv/02mzdv5sEHH+Sqq67qdR+0t7fjdruDV3t9Ph9FRUUsX74cYwxLliwJq659Hc8R9BaX/Px8Vq9eTUlJCTU1NZw6dYqmpiba2tp6fCVHWVkZhw8fZsaMGdxxxx3k5eVhjOH48eM8//zzrF69GoAvfOEL3Q6u4/F4KCsr4wc/+AE33HADCxcu5PXXX2fLli3k5eVx44032q6z0+nkyiuvxLKs4Ciue/bsobS0lMTExH5NHFWvLx46LoaG4vxPUVFRjB07lpdeeom6ujo8Hg+HDx9mw4YNbNiwoU8XamJiYkhPTyczM5NPPvmEyspKxo0b12Nb0dTUxIYNGzhw4ADXXXcdhw8f5sYbb6ShoYFf/OIXjBgxggULFjB37tyzuvx1ds899zB//nxSUlLYvn07f/zjH5k1a9ZZ8zmdTjIzM/nxj3/MvHnzSExMxOfzUVJSwrp163jkkUf6vU2JjIxk0aJFvPXWWzzyyCNER0cTHR1NS0tLcATNiRMndvvYREJCAikpKQwaNIja2lq2bNmCx+Nh4cKFrF27lra2tmCb2vXCiM/no6amhnfeeYcf/OAH1NfXc++997Jw4UImTZrU4/+D0+nksssu45lnnuGWW25hx44dJCcnM2TIkDMGmglHinXPwjJxLC4u5vjx4wwePJiMjAyqqqo4cOAAMTExJCYm2jYSHTq6HBw9epSlS5cya9asS/Yq9IUQERHBmDFjGDVqFO+//z7PPvssV199NVVVVVRUVDBixIhuu3oMBK2trbhcLrxeLydOnGDXrl2sX7+e++67j0GDBtleje5odDr+ofvafaC5uZkDBw6wYsUKbrjhBqZMmWLbPahj9K3p06cTGxtLe3t7MJlqb2+noqKCurq6C/rOw88iOjradnj2Dj6fLzgiX8cIv7t372bFihVER0d3uw/S0tLIysqitLSUtWvXUl5eTlFREYWFhRQVFZGamspVV13FLbfc0m0b43a7qa2tDQ6S4HQ6McZQXV3Nvn37WLhw4Vnf63a7qaioYOPGjSxZsiTYLdPj8QQHk+kYLa2/qV6HNx0XQ0Nx7p7L5Qo+L9XxjsuO5DoqKqpPjxY0NDSQm5vL7NmzeeGFF5g0aVKPd3A7RqRuaGigsLCQO+64g9tuuw2Xy0VlZSWFhYW8++677Ny5kzvvvJPs7OzgcqKiosjIyCA5OZnjx4/z1ltvUVdXx7Fjx2hsbGTKlClnra/T6SQ9PZ3Zs2cHe7EUFxdTXFxMTk4Oo0aN6vfn0SMiIkhPT+d73/sepaWl1NTUsH//frZv3843vvENbr/9drKzs7utozExMYwYMYLdu3ezY8cOSktLqa6u5qmnnsLtdjN9+nRmz5591jY2NTVx6NAh3n33Xd58802MMXzrW99i0aJFjBo1ioSEBNt973A4yMzMZMiQIUyePJnIyEhee+014uPjexxYJhwo1j0Ly8Sx4+rx/v37McZQUVFBUVERixcvJjc3t9erPu3t7RQVFfHmm28yfPhw5s+fz+jRo8Pi5OxiYYxh3LhxzJo1iyNHjrBq1SoKCwtpaWkhJSWFvLy8M95nM1BERESQkpJCfn4+DQ0NREdHU1tby4QJE5g6dSrR0dG25b1eLx6PB4fDQXNzM0lJSb3Wy5aWFo4fP87KlStxOp0sWLCAzMzMXu/qdLzvzufzBU+kIyIiiI2NJT09nfT09H4/EHZmWRbR0dHddlHsKiMjg9tvv50xY8YwYsQInE4nqamprF69msjIyG5jOnjwYK655hree+898vPzKSgo4OTJkzQ1NTF37lxmzZpFXl4el19+ebflO96hlJqayuHDh1m7di1FRUVYlkVra2uPyYrX6+Xjjz9m2LBhjBkzBsuyOHnyJPn5+YwcObJPd1gvNNXr8KfjYmgozj0zxgS7h3aM/NjX91TW1NTg8/kYP34848aNY8WKFWd0Ne8qMjKSK6+8ktjYWGJiYpgyZQozZszA5XJx+vRphgwZwu7du6mtraWhoeGM9jcqKorhw4ezbNkyysvL2bZtGx6Ph/j4eBYsWMCoUaO63baOu0rgv+h37NgxTpw4wezZs8NmBHmXy8WsWbNoamqivLwcgMOHD7N48WImTZrUYz1zuVxcffXVAMERqUeMGMHgwYMZMWIEs2bN6nbMiu3bt7Nu3To++ugjWltbWbp0KUuXLmXYsGG9Hhc6dCRhHT1JKisrWbNmDQ8++GCfLsT0F8W6e2GZOKalpZGamsqJEyc4duwYNTU1ZGdn8+CDDzJ06NBeRwZqbm5mx44dFBYW8v3vf5+pU6f2+MyS9Cw7O5s5c+ZQU1PDm2++ydGjR4mLi2PJkiXMnz/f9qH2S5XD4WD69Ol8/PHHlJSUkJKSwqRJk7jrrrv6VMdcLhcJCQkkJydTX1/f64AjHo+HEydOsGnTJjZv3szDDz/c7TuDuuN2uykpKWHQoEGkpaUFT67T0tIYO3YskyZN6veEpbOO0fP6sk45OTl86UtfCt71c7vdjB07lokTJ/aYOEZFRbFo0SIiIyPZunUrJSUlZGdnk5CQwP3338/06dNtHyJ3uVxkZWWxcOFCtm7dyu7du4Ov7ujuRAT8V7GTk5NJTEzkgw8+oLq6GsuyKCoqoqCggMWLF5ORkdHvr+ZQvQ5/Oi6GhuLcO8uyOHXqFNXV1T0+U95VW1tb8MXkDQ0NDB8+3HZEVZfLFbyg53K5zjjpTUxM5IorrmDChAlUVFScMbAa+JPOoUOH8vDDD/Pqq69SW1tLVlYW06ZNY9asWX1KAmtqaigoKKCxsTGYBIQLh8NBYmIiHo+HjIwMMjMzGTdunO3FiYiICObOncuYMWMoLi6msrISp9PJuHHjGDJkCLGxsd22mx988AHvvPMOlmVx++23c//995OZmXnOF+fGjBnDgQMHAP++feWVV1iyZAlZWVlhfaFPsT5bWCaOcXFx3Hzzzdx44420tLTQ3NxMTExMn5+pa2xsZMeOHcyZM4dly5aRlpZ2gdf40jVq1Cgeeughli5dSlRUFE6nk4SEhH7v699fHA4H3/zmN7npppswxgRfJdDXq8lJSUlcddVVjBo1ipycnF7L1dTUsGHDBt544w3uvPNObr/99j6fFA8ePJgHHniAiRMnkpOTExz0JS0tjeTk5LA7uTbGMHjw4D79n3e8U7GDy+Vi6NCh/PznP7e9gzd+/HjGjx/PV7/6Verr64ODtPQ2mAv4Dwapqan8/Oc/p6ioCI/HQ2xsbPCZp+72pTGGxMRE/vM//5Nnn32WV155hYSEBK688koee+wxcnJyet3WUFC9Dn86LoaG4ty7jtcmRUZG9vluSEpKCjt27GDjxo2MHj2an/70p732MOg8gFZXERERxMXF9dgFz+FwBAfz+Sx2796Nz+dj2rRpYZv4OxwOkpOTGT9+fJ/OyTr217Bhw/r8HVFRUUybNo0rr7zyrNE8z8V9993HH/7wB9566y327duHy+XiwIEDF00PEcX6n8y5PAsyY8YMa+fOnf/yl15oXq+XxsZGjDG99gm+UGbMmMHOnTs/0xdfLHEOF8aYXZZlzfgsZcM91idOnAi+EP3WW2/tc3eFC+VSjnUo9OU1Ix0u5VirXoeejouhEQ5xhgtfp1tbW2ltbcUYQ1JSUp+XfS5tYH9atWoVycnJTJs2rddXcFzK7Yfb7cayLJxO57+cdHQ8G3vq1Cm8Xi/Z2dnnnBwp1n3zr8barq0OyzuO/6qOW8vARdFAifQkJyeHIUOGYFlWWD8LIH2j9shP9Tr0dFwMjYES5+jo6M/U8+hiickNN9xAREREvz9G0N/O10vjwX+XOCEhIZiIXwx3GkPpYon1JfsfcbE0TiJ2IiIi1LjKJUf1un/ouBgaAyXOl/J26oLWhdH1eVS5cC5UrHXkFhEREREREVtKHEVERERERMSWEkcRERERERGxpcRRREREREREbClxFBEREREREVtKHEVERERERMSWEkcRERERERGxpcRRREREREREbClxFBEREREREVtKHEVERERERMSWEkcRERERERGxpcRRREREREREbClxFBEREREREVtKHEVERERERMSWEkcRERERERGxpcRRREREREREbClxFBEREREREVtKHEVERERERMSWEkcRERERERGxpcRRREREREREbClxFBEREREREVtKHEVERERERMSWEkcRERERERGxpcRRREREREREbClxFBEREREREVtKHEVERERERMSWEkcRERERERGxpcRRREREREREbClxFBEREREREVvGsqy+z2xMFXDiwq3OJWWYZVlpn6Wg4nzOFOvQUaxDR7EOHcU6NBTn0FGsQ0exDh3FOjR6jPM5JY4iIiIiIiIy8KirqoiIiIiIiNhS4igiIiIiIiK2lDiKiIiIiIiILSWOIiIiIiIiYkuJo4iIiIiIiNhS4igiIiIiIiK2lDiKiIiIiIiILSWOIiIiIiIiYkuJo4iIiIiIiNj6/7Afv0O8nnTSAAAAAElFTkSuQmCC\n"},"metadata":{},"output_type":"display_data"}]},{"cell_type":"code","metadata":{"cell_id":"00012-0c21d530-14ec-47d2-9d86-a1b022fdbb7f","execution_millis":5,"output_cleared":false,"source_hash":"8abbddd9","tags":[],"execution_start":1607990492962,"deepnote_cell_type":"code"},"source":"def convert_labels(labels):\n    converted_labels=[]\n    for i,label in enumerate(labels):\n        n_label = np.zeros((5, 11), dtype='int')\n        for j,digit in enumerate(label):\n            if digit == \"10\":\n                n_digit = \".\"\n            else:\n                n_digit = int(digit)\n\n            n_label[j][n_digit] = 1\n        converted_labels.append(n_label)\n    y_temp = np.array(converted_labels)\n    print(y_temp.shape)    \n    y1 = y_temp[:, 0, :]\n    y2 = y_temp[:, 1, :]\n    y3 = y_temp[:, 2, :]\n    y4 = y_temp[:, 3, :]\n    y5 = y_temp[:, 4, :]\n\n                \n\n            \n    #converted_labels.append(n_label)\n    return [y1,y2,y3,y4,y5]","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00013-ac29dad8-493b-4e1c-b911-d606707fe6da","execution_millis":1367,"output_cleared":false,"source_hash":"4851664e","tags":[],"execution_start":1607990498355,"deepnote_cell_type":"code"},"source":"labels=convert_labels(y_train)\n#y_train=np.array(labels)\nprint((np.array(labels)).shape)","execution_count":null,"outputs":[{"name":"stdout","text":"(56000, 5, 11)\n(5, 56000, 11)\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"cell_id":"00015-d92dbede-c5a3-492b-b395-9e1a14edaace","execution_millis":3,"output_cleared":false,"source_hash":"5f5effa8","tags":[],"execution_start":1607990500682,"deepnote_cell_type":"code"},"source":"def convert_label(label):\n    n_label = \"\"\n    for digit in label:\n        if np.argmax(digit) == 10:\n            n_digit = \".\"\n        else:\n            n_digit = str(np.argmax(digit))\n        n_label += n_digit\n    return n_label","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00009-023fe856-db62-442a-a5c2-10a540c220ff","execution_millis":436,"output_cleared":false,"source_hash":"1f55a56a","tags":[],"execution_start":1607858229458,"deepnote_cell_type":"code"},"source":"model = get_model(input_shape=(64,64, 1))","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00010-c67df7bf-341e-4aed-8397-67a3aa4abeb7","execution_millis":4,"output_cleared":false,"source_hash":"4e6a3b95","tags":[],"execution_start":1607858232626,"deepnote_cell_type":"code"},"source":"model.summary()","execution_count":null,"outputs":[{"name":"stdout","text":"Model: \"model_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            (None, 64, 64, 1)    0                                            \n__________________________________________________________________________________________________\nbatch_normalization_1 (BatchNor (None, 64, 64, 1)    4           input_1[0][0]                    \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 64, 64, 48)   1248        batch_normalization_1[0][0]      \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 48)   0           conv2d_1[0][0]                   \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 32, 32, 48)   0           max_pooling2d_1[0][0]            \n__________________________________________________________________________________________________\nbatch_normalization_2 (BatchNor (None, 32, 32, 48)   192         dropout_1[0][0]                  \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 32, 32, 64)   76864       batch_normalization_2[0][0]      \n__________________________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D)  (None, 31, 31, 64)   0           conv2d_2[0][0]                   \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 31, 31, 64)   0           max_pooling2d_2[0][0]            \n__________________________________________________________________________________________________\nbatch_normalization_3 (BatchNor (None, 31, 31, 64)   256         dropout_2[0][0]                  \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 31, 31, 128)  204928      batch_normalization_3[0][0]      \n__________________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D)  (None, 15, 15, 128)  0           conv2d_3[0][0]                   \n__________________________________________________________________________________________________\ndropout_3 (Dropout)             (None, 15, 15, 128)  0           max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\nbatch_normalization_4 (BatchNor (None, 15, 15, 128)  512         dropout_3[0][0]                  \n__________________________________________________________________________________________________\nconv2d_4 (Conv2D)               (None, 15, 15, 160)  512160      batch_normalization_4[0][0]      \n__________________________________________________________________________________________________\nmax_pooling2d_4 (MaxPooling2D)  (None, 14, 14, 160)  0           conv2d_4[0][0]                   \n__________________________________________________________________________________________________\ndropout_4 (Dropout)             (None, 14, 14, 160)  0           max_pooling2d_4[0][0]            \n__________________________________________________________________________________________________\nbatch_normalization_5 (BatchNor (None, 14, 14, 160)  640         dropout_4[0][0]                  \n__________________________________________________________________________________________________\nconv2d_5 (Conv2D)               (None, 14, 14, 192)  768192      batch_normalization_5[0][0]      \n__________________________________________________________________________________________________\nmax_pooling2d_5 (MaxPooling2D)  (None, 7, 7, 192)    0           conv2d_5[0][0]                   \n__________________________________________________________________________________________________\ndropout_5 (Dropout)             (None, 7, 7, 192)    0           max_pooling2d_5[0][0]            \n__________________________________________________________________________________________________\nbatch_normalization_6 (BatchNor (None, 7, 7, 192)    768         dropout_5[0][0]                  \n__________________________________________________________________________________________________\nconv2d_6 (Conv2D)               (None, 7, 7, 192)    921792      batch_normalization_6[0][0]      \n__________________________________________________________________________________________________\nmax_pooling2d_6 (MaxPooling2D)  (None, 6, 6, 192)    0           conv2d_6[0][0]                   \n__________________________________________________________________________________________________\ndropout_6 (Dropout)             (None, 6, 6, 192)    0           max_pooling2d_6[0][0]            \n__________________________________________________________________________________________________\nflatten_1 (Flatten)             (None, 6912)         0           dropout_6[0][0]                  \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 1024)         7078912     flatten_1[0][0]                  \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 1024)         1049600     dense_1[0][0]                    \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 11)           11275       dense_2[0][0]                    \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 11)           11275       dense_2[0][0]                    \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 11)           11275       dense_2[0][0]                    \n__________________________________________________________________________________________________\ndense_6 (Dense)                 (None, 11)           11275       dense_2[0][0]                    \n__________________________________________________________________________________________________\ndense_7 (Dense)                 (None, 11)           11275       dense_2[0][0]                    \n==================================================================================================\nTotal params: 10,672,443\nTrainable params: 10,671,257\nNon-trainable params: 1,186\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"cell_id":"00012-ebef4c64-ca42-4878-bb47-daba13e44be4","execution_millis":664622,"output_cleared":true,"source_hash":null,"tags":[],"execution_start":1607837727336,"deepnote_cell_type":"code"},"source":"optimizer = Adam(lr=1e-3)\nmodel.compile(optimizer, loss='categorical_crossentropy')\nmodel.fit(X_train, labels, epochs=500, verbose=0,steps_per_epoch=1)\nteste_out = model.predict(X_train)\n#model.fit()","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00013-cc8d6b1d-a106-4647-915f-5b08768dde49","execution_millis":0,"output_cleared":false,"source_hash":"1b9b7dbf","tags":[],"deepnote_cell_type":"code"},"source":"output = convert_output(teste_out)\n\nrows_to_plot = 4\ncols_to_plot = 2\nprint(output)\nprint(y_train)\nf = plt.figure(figsize=(12, 6))\n\nfor i, (pred, img) in enumerate(zip(output, test_imgs)):\n    f.add_subplot(rows_to_plot, cols_to_plot, i+1)\n    plt.title(\"Predicted: \" + pred)\n    plt.imshow(img[:, :, 0], cmap='gray')\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00021-a9b75002-5f4a-44ce-be93-93d24b4fbd42","output_cleared":false,"source_hash":"6a4681c4","tags":[],"execution_millis":5,"execution_start":1607933842268,"deepnote_cell_type":"code"},"source":"import random\ndef generator(size,train_data,train_labels, batch_size=32):\n    \"\"\"\n    This generator receives mnist digits and labels and returns a batch for training\n\n    Input:\n    numbers - array with mnist images.\n    number_labels - array with mnist labels.\n\n    Arguments:\n    batch_size - size of the mini batch\n\n    Output:\n    X_train and y_train\n    \"\"\"\n    while True:  # Loop forever so the generator never terminates\n\n        images = []\n        labels = []\n\n        #for batch_sample in range(batch_size):\n        index=random.sample(range(0,size), batch_size)\n        xs,ys = train_data[index],train_labels[index]\n            \n            # Here we will convert the label to a format that Keras API can process:\n        for label in ys:\n            n_label = np.zeros((5, 11), dtype='int')\n            for i, digit in enumerate(label):\n                if digit == \".\":\n                    n_digit = 10\n                else:\n                    n_digit = int(digit)\n\n                n_label[i][n_digit] = 1\n                \n            #images.append(img)\n            #labels.append(label)\n            labels.append(n_label)\n\n        X_train = np.array(xs)\n        #print(X_train.shape)\n        #if len(X_train.shape) == 3:\n        #X_train = np.expand_dims(X_train, 0)\n        #print(X_train.shape)\n\n        y_temp = np.array(labels)\n        \n        y1 = y_temp[:, 0, :]\n        y2 = y_temp[:, 1, :]\n        y3 = y_temp[:, 2, :]\n        y4 = y_temp[:, 3, :]\n        y5 = y_temp[:, 4, :]\n\n        yield X_train, [y1, y2, y3, y4, y5]","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00022-9428fb2b-b568-4140-8a19-82379762ce66","output_cleared":false,"source_hash":"dd1d1d1","execution_millis":535,"execution_start":1607858875695,"deepnote_cell_type":"code"},"source":"model = get_model(input_shape=(64,64, 1))\noptimizer = Adam(lr=1e-3)\nmodel.compile(optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n\ntrn_generator = generator(56000,X_train, y_train, batch_size=128)\nval_generator = generator(11190,X_val, y_val, batch_size=128)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00023-74cf933b-de11-499b-a73e-4f221f6b31e1","output_cleared":false,"source_hash":"253fab6e","execution_millis":43814657,"execution_start":1607858880273,"deepnote_cell_type":"code"},"source":"model.fit_generator(trn_generator,\n                    epochs=5,\n                    steps_per_epoch=780,\n                    validation_data=val_generator,\n                    validation_steps=780,\n                    verbose=1,\n                    shuffle=True)\n","execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/5\n780/780 [==============================] - 8788s 11s/step - loss: 3.2734 - dense_17_loss: 0.9028 - dense_18_loss: 0.8674 - dense_19_loss: 0.6819 - dense_20_loss: 0.4817 - dense_21_loss: 0.3396 - dense_17_categorical_accuracy: 0.6825 - dense_18_categorical_accuracy: 0.6963 - dense_19_categorical_accuracy: 0.7634 - dense_20_categorical_accuracy: 0.8350 - dense_21_categorical_accuracy: 0.8879 - val_loss: 0.6118 - val_dense_17_loss: 0.1071 - val_dense_18_loss: 0.1077 - val_dense_19_loss: 0.0862 - val_dense_20_loss: 0.0668 - val_dense_21_loss: 0.0426 - val_dense_17_categorical_accuracy: 0.9694 - val_dense_18_categorical_accuracy: 0.9694 - val_dense_19_categorical_accuracy: 0.9778 - val_dense_20_categorical_accuracy: 0.9825 - val_dense_21_categorical_accuracy: 0.9875\nEpoch 2/5\n780/780 [==============================] - 8755s 11s/step - loss: 0.3774 - dense_17_loss: 0.1019 - dense_18_loss: 0.0934 - dense_19_loss: 0.0804 - dense_20_loss: 0.0572 - dense_21_loss: 0.0444 - dense_17_categorical_accuracy: 0.9722 - dense_18_categorical_accuracy: 0.9751 - dense_19_categorical_accuracy: 0.9773 - dense_20_categorical_accuracy: 0.9846 - dense_21_categorical_accuracy: 0.9869 - val_loss: 0.0759 - val_dense_17_loss: 0.0429 - val_dense_18_loss: 0.0429 - val_dense_19_loss: 0.0238 - val_dense_20_loss: 0.0144 - val_dense_21_loss: 0.0122 - val_dense_17_categorical_accuracy: 0.9900 - val_dense_18_categorical_accuracy: 0.9898 - val_dense_19_categorical_accuracy: 0.9950 - val_dense_20_categorical_accuracy: 0.9958 - val_dense_21_categorical_accuracy: 0.9968\nEpoch 3/5\n780/780 [==============================] - 8751s 11s/step - loss: 0.2202 - dense_17_loss: 0.0613 - dense_18_loss: 0.0521 - dense_19_loss: 0.0502 - dense_20_loss: 0.0327 - dense_21_loss: 0.0239 - dense_17_categorical_accuracy: 0.9840 - dense_18_categorical_accuracy: 0.9864 - dense_19_categorical_accuracy: 0.9863 - dense_20_categorical_accuracy: 0.9914 - dense_21_categorical_accuracy: 0.9933 - val_loss: 1.7545 - val_dense_17_loss: 0.2762 - val_dense_18_loss: 0.7950 - val_dense_19_loss: 0.4678 - val_dense_20_loss: 0.0751 - val_dense_21_loss: 0.0191 - val_dense_17_categorical_accuracy: 0.8978 - val_dense_18_categorical_accuracy: 0.7639 - val_dense_19_categorical_accuracy: 0.8758 - val_dense_20_categorical_accuracy: 0.9831 - val_dense_21_categorical_accuracy: 0.9955\nEpoch 4/5\n780/780 [==============================] - 8761s 11s/step - loss: 0.1819 - dense_17_loss: 0.0496 - dense_18_loss: 0.0438 - dense_19_loss: 0.0394 - dense_20_loss: 0.0297 - dense_21_loss: 0.0194 - dense_17_categorical_accuracy: 0.9874 - dense_18_categorical_accuracy: 0.9885 - dense_19_categorical_accuracy: 0.9900 - dense_20_categorical_accuracy: 0.9923 - dense_21_categorical_accuracy: 0.9946 - val_loss: 0.0687 - val_dense_17_loss: 0.0214 - val_dense_18_loss: 0.0211 - val_dense_19_loss: 0.0152 - val_dense_20_loss: 0.0114 - val_dense_21_loss: 0.0055 - val_dense_17_categorical_accuracy: 0.9941 - val_dense_18_categorical_accuracy: 0.9944 - val_dense_19_categorical_accuracy: 0.9967 - val_dense_20_categorical_accuracy: 0.9973 - val_dense_21_categorical_accuracy: 0.9985\nEpoch 5/5\n301/780 [==========>...................] - ETA: 1:13:03 - loss: 0.1605 - dense_17_loss: 0.0401 - dense_18_loss: 0.0453 - dense_19_loss: 0.0318 - dense_20_loss: 0.0250 - dense_21_loss: 0.0183 - dense_17_categorical_accuracy: 0.9899 - dense_18_categorical_accuracy: 0.9887 - dense_19_categorical_accuracy: 0.9915 - dense_20_categorical_accuracy: 0.9928 - dense_21_categorical_accuracy: 0.9951","output_type":"stream"},{"name":"stdout","text":"780/780 [==============================] - 8756s 11s/step - loss: 0.1564 - dense_17_loss: 0.0418 - dense_18_loss: 0.0416 - dense_19_loss: 0.0327 - dense_20_loss: 0.0221 - dense_21_loss: 0.0181 - dense_17_categorical_accuracy: 0.9897 - dense_18_categorical_accuracy: 0.9894 - dense_19_categorical_accuracy: 0.9915 - dense_20_categorical_accuracy: 0.9939 - dense_21_categorical_accuracy: 0.9953 - val_loss: 0.0395 - val_dense_17_loss: 0.0222 - val_dense_18_loss: 0.0158 - val_dense_19_loss: 0.0173 - val_dense_20_loss: 0.0050 - val_dense_21_loss: 0.0034 - val_dense_17_categorical_accuracy: 0.9947 - val_dense_18_categorical_accuracy: 0.9966 - val_dense_19_categorical_accuracy: 0.9953 - val_dense_20_categorical_accuracy: 0.9984 - val_dense_21_categorical_accuracy: 0.9990\n","output_type":"stream"},{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"<keras.callbacks.callbacks.History at 0x7f12fae65b90>"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00024-2f106022-5b4f-4a95-8470-ebb8b1e8e711","output_cleared":false,"source_hash":"1ae70af7","execution_millis":1,"deepnote_cell_type":"code"},"source":"y_labels=convert_labels(y_test)\n\n#print(y_labels.shape)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00025-2dcd3a90-2ee3-46ea-9dce-04593da3392e","output_cleared":false,"source_hash":"d5a8b8df","execution_millis":1754,"deepnote_cell_type":"code"},"source":"model.evaluate(X_test,y_labels)#predict on trianing data--overfit\ntest_pred = model.predict(X_test)\ntest_pred_labels = convert_output(test_pred)\ntest_true_labels = convert_output(y_labels)\nprint(test_pred_labels)\nprint(test_true_labels)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00026-1342b63c-6b57-48f3-a260-af0f008130ce","output_cleared":false,"source_hash":"e4ac3534","execution_millis":4,"deepnote_cell_type":"code"},"source":"accuracy = accuracy_score(test_true_labels, test_pred_labels)\nprint( \"Accuracy: \", accuracy*100, \"%\")","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00027-4ef2ceb9-616d-416d-9ac2-fc46aa7d22ed","output_cleared":false,"source_hash":"864c3c88","execution_millis":7,"execution_start":1607933867960,"deepnote_cell_type":"code"},"source":"def convert_labels_back(labels):#concerted . to 10 for writing csv\n    converted_labels=[]\n    for i,label in enumerate(labels):\n        n_label = \"\"\n        for j,digit in enumerate(label):\n            if digit == \".\":\n                n_digit = \"10\"\n            else:\n                n_digit = str(digit)\n\n            n_label+=n_digit\n        converted_labels.append(n_label)\n    return converted_labels","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00028-dc4ff98f-56b5-4b2c-9679-1100cf5b983d","output_cleared":true,"source_hash":null,"execution_millis":197,"execution_start":1607933851359,"deepnote_cell_type":"code"},"source":"input_labels=convert_labels_back(test_pred_labels)\nprint(test_pred_labels)\nprint(input_labels)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00026-fd263c39-fffb-49b6-ae1c-17f48b2ffcfd","output_cleared":false,"source_hash":"c164ae74","execution_millis":1,"execution_start":1607933857798,"deepnote_cell_type":"code"},"source":"import csv\nimport os.path\ndef write_csv(output,filename):\n    headers = ['Id', 'Label']\n\n    for i,label in enumerate(output):\n       \n        print(i)\n        row=[str(i),label]\n        print(row)\n            #if i>13999:\n\n        with open(filename, 'a') as f:\n            file_is_empty = os.stat(filename).st_size == 0\n            writer = csv.writer(f, delimiter =\",\", lineterminator='\\n')\n\n            if file_is_empty:\n                writer.writerow(headers)\n            writer.writerow(row)\n        ","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00028-83b632d5-a85b-4051-bbe1-2ad22bcdc7fd","output_cleared":false,"source_hash":"9906989e","execution_millis":71,"deepnote_cell_type":"code"},"source":"write_csv(input_labels,\"test.csv\")","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00032-0df1be7e-fd6c-4fb9-b193-d12bcc3c9d0e","output_cleared":false,"source_hash":"725ee01d","execution_millis":221379,"execution_start":1607930221926,"deepnote_cell_type":"code"},"source":"kaggle_pred = model.predict(X_kaggle)\nkaggle_pred_labels = convert_output(kaggle_pred)\nKaggle_input_labels=convert_labels_back(kaggle_pred_labels)\nwrite_csv(Kaggle_input_labels,\"kaggle3.csv\")","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00024-9c8a0444-c0d1-44c6-88da-decacb017122","output_cleared":false,"source_hash":"9620fd09","execution_millis":640448,"deepnote_cell_type":"code"},"source":"#model.evaluate(X_val, y_train)\nmodel.fit(X_train, y_labels, epochs=500, verbose=0,steps_per_epoch=1)\nteste_out = model.predict(X_val)\noutput = convert_output(teste_out)\n\nrows_to_plot = 4\ncols_to_plot = 2\nprint(output)\nprint(y_val)\nf = plt.figure(figsize=(12, 6))\n\nfor i, (pred, img) in enumerate(zip(output, X_val)):\n    f.add_subplot(rows_to_plot, cols_to_plot, i+1)\n    plt.title(\"Predicted: \" + pred)\n    plt.imshow(img[:, :, 0], cmap='gray')\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# HYPERPARAMETER OPTIMIZATION","metadata":{"tags":[],"cell_id":"00037-7d21ca75-c1cd-46c4-be62-e4761ada6784","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00038-e4cf1a6c-2f1c-4e05-9fe3-97acd1582840","output_cleared":false,"source_hash":"8a168502","deepnote_cell_type":"code"},"source":"import tensorflow as tf\nprint(\"Tensorflow version: \" + tf.__version__)\nimport keras\nimport keras.backend as K\nimport random\n\nfrom keras.models import Model\nfrom keras.layers import Input, BatchNormalization, Convolution2D, Dense, Dropout, MaxPooling2D, Flatten\nfrom keras.layers import AveragePooling2D, GlobalAveragePooling2D, concatenate, UpSampling2D, Conv2DTranspose\n\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.utils.vis_utils import model_to_dot\n\nfrom talos.utils import lr_normalizer\n\n\nh5f = h5py.File('MNIST_synthetic.h5','r')\n\nprint(h5f.keys())\n# Extract the datasets\nX_train = h5f['train_dataset'][:-50]       # remove\ny_train = h5f['train_labels'][:-50]\nX_test=h5f['train_dataset'][-50:] \ny_test=h5f['train_labels'][-50:]\n#X_val = h5f['valid_dataset'][:]\n#y_val = h5f['valid_labels'][:]\nX_kaggle = h5f['test_dataset'][:]\n#y_test = h5f['test_labels'][:]\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n# Close the file\nh5f.close()\n\nprint('Training set', X_train.shape, y_train.shape)\nprint('Validation set', X_val.shape, y_val.shape)\nprint('Test set', X_test.shape)\n\nlabels=convert_labels(y_train)\n\n\n\"\"\"\n    model.add(Dense(y_train.shape[1],\n                    activation=params['last_activation']))\n\n    model.compile(optimizer=params['optimizer'](lr=lr_normalizer(params['lr'], params['optimizer'])),\n                  loss=params['loss'],\n                  metrics=['acc'])\n\n    out = model.fit(x_train, y_train,\n                    batch_size=params['batch_size'],\n                    epochs=params['epochs'],\n                    verbose=0,\n                    validation_data=[x_val, y_val])\n\"\"\"","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00025-b7f2527e-20c0-4fbb-8881-ad214bb08c47","output_cleared":false,"source_hash":"573dcaca","deepnote_cell_type":"code"},"source":"def get_model(input_shape=(28, 28*5, 3), p=0.5, n_class=11):\n\n    inputs = Input(((input_shape[0], input_shape[1], input_shape[2])))\n    \n    x = BatchNormalization()(inputs)\n    x = Convolution2D(48, 5, activation='relu', padding='same', strides=(1, 1))(x)\n    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n    x = Dropout(p/4)(x)\n    \n    x = BatchNormalization()(x)\n    x = Convolution2D(64, 5, activation='relu', padding='same', strides=(1, 1))(x)\n    x = MaxPooling2D(pool_size=(2, 2), strides=(1, 1))(x)\n    x = Dropout(p/4)(x)\n\n    x = BatchNormalization()(x)\n    x = Convolution2D(128, 5, activation='relu', padding='same', strides=(1, 1))(x)\n    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n    x = Dropout(p/2)(x)\n\n    x = BatchNormalization()(x)\n    x = Convolution2D(160, 5, activation='relu', padding='same', strides=(1, 1))(x)\n    x = MaxPooling2D(pool_size=(2, 2), strides=(1, 1))(x)\n    x = Dropout(p/2)(x)\n\n    x = BatchNormalization()(x)\n    x = Convolution2D(192, 5, activation='relu', padding='same', strides=(1, 1))(x)\n    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n    x = Dropout(p)(x)\n\n    x = BatchNormalization()(x)\n    x = Convolution2D(192, 5, activation='relu', padding='same', strides=(1, 1))(x)\n    x = MaxPooling2D(pool_size=(2, 2), strides=(1, 1))(x)\n    x = Dropout(p)(x)\n    \n    # I had to remove this part because the input size we have is too small for a network this deep.\n    # Another alternative would have been change the maxpool strides.\n    \n    #x = BatchNormalization()(x)\n    #x = Convolution2D(192, 5, activation='relu', padding='same', strides=(1, 1))(x)\n    #x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n    #x = Dropout(p)(x)\n    #x = BatchNormalization()(x)\n    #x = Convolution2D(192, 5, activation='relu', padding='same', strides=(1, 1))(x)\n    #x = MaxPooling2D(pool_size=(2, 2), strides=(1, 1))(x)\n    #x = Dropout(p)(x)\n    \n    x = Flatten()(x)\n    x = Dense(1024, activation='relu')(x) # I also reduced the number of activations\n    x = Dense(1024, activation='relu')(x)\n    \n    c1 = Dense(n_class, activation='softmax')(x)\n    c2 = Dense(n_class, activation='softmax')(x)\n    c3 = Dense(n_class, activation='softmax')(x)\n    c4 = Dense(n_class, activation='softmax')(x)\n    c5 = Dense(n_class, activation='softmax')(x)\n    \n    output = [c1, c2, c3, c4, c5]\n    \n    model = Model(inputs=inputs, outputs=output)\n\n    return model\n\n\ndef generator(size,train_data,train_labels, batch_size=32):\n    \"\"\"\n    This generator receives mnist digits and labels and returns a batch for training\n\n    Input:\n    numbers - array with mnist images.\n    number_labels - array with mnist labels.\n\n    Arguments:\n    batch_size - size of the mini batch\n\n    Output:\n    X_train and y_train\n    \"\"\"\n    while True:  # Loop forever so the generator never terminates\n\n        images = []\n        labels = []\n\n        #for batch_sample in range(batch_size):\n        index=random.sample(range(0,size), batch_size)\n        xs,ys = train_data[index],train_labels[index]\n            \n            # Here we will convert the label to a format that Keras API can process:\n        for label in ys:\n            n_label = np.zeros((5, 11), dtype='int')\n            for i, digit in enumerate(label):\n                if digit == \".\":\n                    n_digit = 10\n                else:\n                    n_digit = int(digit)\n\n                n_label[i][n_digit] = 1\n                \n            #images.append(img)\n            #labels.append(label)\n            labels.append(n_label)\n\n        X_train = np.array(xs)\n        #print(X_train.shape)\n        #if len(X_train.shape) == 3:\n        #X_train = np.expand_dims(X_train, 0)\n        #print(X_train.shape)\n\n        y_temp = np.array(labels)\n        \n        y1 = y_temp[:, 0, :]\n        y2 = y_temp[:, 1, :]\n        y3 = y_temp[:, 2, :]\n        y4 = y_temp[:, 3, :]\n        y5 = y_temp[:, 4, :]\n\n        yield X_train, [y1, y2, y3, y4, y5]\n\n\n\ndef train(X_train, y_train, X_val, y_val, params):\n    model = get_model(input_shape=(64,64, 1))   #static\n    \n    model.compile(params['optimizer'](lr=lr_normalizer(params['lr'], params['optimizer'])), \n    loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n\n    trn_generator = generator(44760,X_train, y_train, batch_size=128)\n    val_generator = generator(11190,X_val, y_val, batch_size=128)\n\n    model.fit_generator(trn_generator,\n                        epochs=params['epoch'],\n                        validation_data=val_generator,\n                        verbose=1,\n                        shuffle=True)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00039-7254441f-420b-45af-9a3d-888ca1deaa47","output_cleared":false,"source_hash":"acb32008","execution_start":1607992002692,"execution_millis":0,"deepnote_cell_type":"code"},"source":"def convert_output(model_output):\n    model_output = np.array(model_output).swapaxes(0, 1)\n    labels = []\n    for output in model_output:\n        label = convert_label(output)\n        labels.append(label)\n\n    return labels\n\n\ndef plot_images(images, nrows, ncols, cls_true, cls_pred=None):\n    \n    # Initialize figure\n    fig, axes = plt.subplots(nrows, ncols, figsize=(16, 2*nrows))\n    \n    # Randomly select nrows * ncols images\n    rs = np.random.choice(images.shape[0], nrows*ncols)\n    \n    # For every axes object in the grid\n    for i, ax in zip(rs, axes.flat): \n        \n        # Pretty string with actual number\n        true_number = ''.join(str(x) for x in cls_true[i] if x != 10)\n        \n        if cls_pred is None:\n            title = \"True: {0}\".format(true_number)\n        else:\n            # Pretty string with predicted number\n            pred_number = ''.join(str(x) for x in cls_pred[i] if x != 10)\n            title = \"True: {0}, Pred: {1}\".format(true_number, pred_number) \n            \n        ax.imshow(images[i,:,:,0], cmap='binary')\n        ax.set_title(title)   \n        ax.set_xticks([]); ax.set_yticks([])\n        \n        \n# Plot some images from the training set\n#plot_images(X_train[:2], 1, 2, y_train[:2])\n\ndef convert_labels(labels):\n    converted_labels=[]\n    for i,label in enumerate(labels):\n        n_label = np.zeros((5, 11), dtype='int')\n        for j,digit in enumerate(label):\n            if digit == \"10\":\n                n_digit = \".\"\n            else:\n                n_digit = int(digit)\n\n            n_label[j][n_digit] = 1\n        converted_labels.append(n_label)\n    y_temp = np.array(converted_labels)\n    print(y_temp.shape)    \n    y1 = y_temp[:, 0, :]\n    y2 = y_temp[:, 1, :]\n    y3 = y_temp[:, 2, :]\n    y4 = y_temp[:, 3, :]\n    y5 = y_temp[:, 4, :]\n\n                \n\n            \n    #converted_labels.append(n_label)\n    return [y1,y2,y3,y4,y5]\n\n\n\ndef convert_label(label):\n    n_label = \"\"\n    for digit in label:\n        if np.argmax(digit) == 10:\n            n_digit = \".\"\n        else:\n            n_digit = str(np.argmax(digit))\n        n_label += n_digit\n    return n_label","execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00041-e08d2f52-af0e-4368-94e0-34137911c160","output_cleared":false,"source_hash":"1f55a56a","deepnote_cell_type":"code"},"source":"model = get_model(input_shape=(64,64, 1))","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"a8721217-44a7-4df2-bf5d-f5f1ba12e950","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}}}